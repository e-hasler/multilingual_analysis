all_tasks = ['aclue', 'acp_bench', 'acp_bench_hard', 'aexams', 'agieval', 'aime', 'anli', 'arabic_leaderboard_complete', 'arabic_leaderboard_light', 'arabicmmlu', 'ArabCulture', 'AraDICE', 'arc', 'arithmetic', 'asdiv', 'babi', 'babilong', 'bangla_mmlu', 'bangla_commonsenseQA', 'bangla_boolQA', 'bangla_piQA', 'bangla_poenbookQA', 'basque_bench', 'basqueglue', 'bbh', 'bbq', 'belebele', 'bertaqa', 'bhs', 'bigbench', 'blimp', 'blimp_nl', 'c4', 'cabbq', 'careqa', 'catalan_bench', 'ceval', 'cmmlu', 'code_x_glue', 'cnn_dailymail_abisee', 'commonsense_qa', 'coqa', 'crows_pairs', 'click', 'darija_bench', 'darijahellaswag', 'darijammlu', 'discrim_eval', 'drop', 'egyhellaswag', 'egymmlu', 'eq_bench', 'esbbq', 'eus_exams', 'eus_proficiency', 'eus_reading', 'eus_trivia', 'evalita_LLM', 'eq-bench_es', 'eq-bench_ca', 'fda', 'fld', 'french_bench', 'galician_bench', 'global_mmlu', 'glue', 'gpqa', 'gsm8k', 'groundcocoa', 'haerae', 'headqa', 'hellaswag', 'hendrycks_ethics', 'hendrycks_math', 'histoires_morales', 'hrm8k', 'humaneval', 'humaneval_infilling', 'icelandic_winogrande', 'ifeval', 'ifeval_es', 'ifeval_ca', 'inverse_scaling', 'japanese_leaderboard', 'jsonschema_bench', 'kbl', 'kmmlu', 'kobest', 'kormedmcqa', 'lambada', 'lambada_cloze', 'lambada_multilingual', 'lambada_multilingual_stablelm', 'leaderboard', 'lingoly', 'llama3', 'libra', 'lm_syneval', 'logiqa', 'logiqa2', 'longbench', 'longbenchv2', 'mastermind', 'mathqa', 'mbpp', 'meddialog', 'medtext', 'mimic_repsum', 'mc_taco', 'med_concepts_qa', 'metabench', 'mediqa_qa2019', 'meqsum', 'mgsm', 'minerva_math', 'mlqa', 'mmlu', 'mmlu_redux', 'mmlu_pro', 'mmlu-pro-plus', 'mmlu_prox', 'mmlusr', 'moral_stories', 'mts_dialog', 'multiblimp', 'mutual', 'noreval', 'nq_open', 'okapi/arc_multilingual', 'okapi/hellaswag_multilingual', 'okapi/truthfulqa_multilingual', 'olaph', 'openbookqa', 'paloma', 'paws-x', 'pile', 'pile_10k', 'piqa', 'pisa', 'polemo2', 'portuguese_bench', 'prost', 'pubmedqa', 'qa4mre', 'qasper', 'race', 'ruler', 'sciq', 'score', 'scrolls', 'simple_cooccurrence_bias', 'siqa', 'slr_bench_group', 'spanish_bench', 'squad_completion', 'squadv2', 'storycloze', 'super_glue', 'swag', 'swde', 'tinyBenchmarks', 'tmmluplus', 'toxigen', 'translation', 'triviaqa', 'truthfulqa', 'truthfulqa-multi', 'turkishmmlu', 'turblimp_core', 'unitxt', 'ulqa', 'unscramble', 'webqs', 'wikitext', 'winogender', 'winogrande', 'wmdp', 'wmt2016', 'wsc273', 'xcopa', 'xnli', 'xnli_eu', 'xquad', 'xstorycloze', 'xwinograd', 'zhoblimp']
non_english_only_tasks = ['aclue', 'aexams', 'agieval', 'arabic_leaderboard_complete', 'arabic_leaderboard_light', 'arabicmmlu', 'ArabCulture', 'AraDICE', 'bangla_mmlu', 'bangla_commonsenseQA', 'bangla_boolQA', 'bangla_piQA', 'bangla_poenbookQA', 'basque_bench', 'basqueglue', 'bbh', 'belebele', 'bertaqa', 'bhs', 'bigbench', 'blimp_nl', 'cabbq', 'careqa', 'catalan_bench', 'ceval', 'cmmlu', 'code_x_glue', 'cnn_dailymail_abisee', 'crows_pairs', 'click', 'darija_bench', 'darijahellaswag', 'darijammlu', 'egyhellaswag', 'egymmlu', 'esbbq', 'eus_exams', 'eus_proficiency', 'eus_reading', 'eus_trivia', 'evalita_LLM', 'eq-bench_es', 'eq-bench_ca', 'french_bench', 'galician_bench', 'global_mmlu', 'haerae', 'headqa', 'histoires_morales', 'hrm8k', 'humaneval', 'humaneval_infilling', 'icelandic_winogrande', 'ifeval_es', 'ifeval_ca', 'japanese_leaderboard', 'jsonschema_bench', 'kbl', 'kmmlu', 'kobest', 'kormedmcqa', 'lambada_multilingual', 'lambada_multilingual_stablelm', 'lingoly', 'llama3', 'libra', 'logiqa', 'logiqa2', 'longbench', 'longbenchv2', 'mbpp', 'meqsum', 'mgsm', 'mlqa', 'mmlu_redux', 'mmlu_prox', 'multiblimp', 'noreval', 'okapi/arc_multilingual', 'okapi/hellaswag_multilingual', 'okapi/truthfulqa_multilingual', 'paws-x', 'pisa', 'polemo2', 'portuguese_bench', 'spanish_bench', 'tmmluplus', 'translation', 'truthfulqa-multi', 'turkishmmlu', 'turblimp_core', 'ulqa', 'wmt2016', 'xcopa', 'xnli', 'xnli_eu', 'xquad', 'xstorycloze', 'xwinograd', 'zhoblimp']